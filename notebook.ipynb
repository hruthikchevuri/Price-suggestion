{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code]\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# %% [code]\ndf_train = pd.read_csv('../input/mercari/train.tsv', sep='\\t')\ndf_test = pd.read_csv('../input/mercari/test.tsv', sep='\\t')\n\n# %% [code]\nprint('Train shape:',df_train.shape)\nprint('Test shape:',df_test.shape)\n\n# %% [code]\ndf_train.head()\n\n# %% [code]\ndf_train.describe\n\n#plot for price distribution\nplt.figure(figsize=(20, 15))\nplt.hist(df_train['price'], bins=50, range=[0,250], label='price')\nplt.title('Train \"price\" distribution')\nplt.xlabel('Price')\nplt.ylabel('No. of items')\nplt.legend()\nplt.show()\n\n#plot for price distribution after log transformation\nplt.figure(figsize=(8,5))\n\nplt.hist(np.log1p(df_train['price']), bins=100)\nplt.title('Train \" log(price+1)\" distribution', fontsize=15)\nplt.xlabel('Price', fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n#snsplot for item condition vs price\nsns.jointplot(x = df_train.item_condition_id, y = df_train['price'])\n\n#plot for county of item_condition\ncount=df_train['item_condition_id'].value_counts()\nplt.figure(figsize=(8,5))\nsns.barplot(count.index[:10],count[:10])\nplt.xlabel('Item_condition_id')\nplt.ylabel('no. of items')\nplt.show()\n\n#plot for shipping\ncount=df_train['shipping'].value_counts()\nplt.figure(figsize=(7,3))\nplt.subplot(1,2,1)\nsns.barplot(count.index,count)\nplt.xlabel('Shipping')\nplt.ylabel('Count')\nplt.title('Shipping vs Count')\nplt.subplot(1,2,2)\nlabels = ['0','1']\nsizes = count\ncolors = ['green','red']\nexplode = (0.1, 0)  # explode 1st slice\n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=140)\nplt.axis('equal')\nplt.show()\n\n\none_shipping=np.log(df_train.loc[df_train['shipping']==1,'price']+1)\nzero_shipping=np.log(df_train.loc[df_train['shipping']==0,'price']+1)\nsns.distplot(one_shipping,label='shipping = 1')\nsns.distplot(zero_shipping,label='shipping = 0')\nplt.title('distribution plot for shipping = 1 and shipping = 0')\nplt.xlabel(\"log(price+1)\")\nplt.legend()\nplt.show()\n\n\n#plot for brand and price\ntop_50_expensive_brand=df_train.groupby(['brand_name'])['price'].mean().sort_values()[:15]\nsns.set(rc={'figure.figsize':(8,5)})\ntop_50_expensive_brand.reset_index()\nax=sns.barplot(y='brand_name',x='price',data=top_50_expensive_brand.reset_index())\nax.set_xlabel('Average Price of products',fontsize=15)\nax.set_ylabel('Brand',fontsize=15)\nplt.show()\n\n#worclouds\n\nfrom wordcloud import WordCloud\n\ncloud = WordCloud(width=1440,height=1080).generate(\" \".join(df_train['name'].astype(str)))\nplt.figure(figsize=(18, 15))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()\n\n\n# %% [code]\n#AS WE CAN SEE ABOVE WE HAVE 3 PARTS IN CATEGORY NAME. FIRST WE'LL SPLIT THEM.\ndef transform_category_name(category_name):\n    try:\n        main, sub1, sub2= category_name.split('/')\n        return main, sub1, sub2\n    except:\n        return np.nan, np.nan, np.nan\n\ndf_train['category_main'], df_train['category_sub1'], df_train['category_sub2'] = zip(*df_train['category_name'].apply(transform_category_name))\n\n# %% [code]\n#NOW WE WILL FILL NULLS WITH FILLNA\ndf_train['category_main']=df_train['category_main'].fillna(\"No main Categ given\")\ndf_train['category_sub1']=df_train['category_sub1'].fillna(\"No sub1 Categ given\")\ndf_train['category_sub2']=df_train['category_sub2'].fillna(\"No sub2 Categ given\")\ndf_train['brand_name']=df_train['brand_name'].fillna(\"No brand given\")\n#WE TRIED FILLING WITH MODE,MEAN,FFILL,BFILL BUT FILLING WITH NOT GIVEN RESPECTIVELY GAVE BEST RESULTS\n\n# %% [code]\n#import re\n\n#def contracting(sentence):         #In this func few contractions are substituted instead of whole word\n #   sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n #   sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n #   sentence = re.sub(r\"\\'s\", \" is\", sentence)\n #   sentence = re.sub(r\"\\'re\", \" are\", sentence)\n #   sentence = re.sub(r\"\\'ve\", \" have\", sentence)   #THIS FUNC DECREASED OUR ACCURACY SO WE COMMENTED IT \n #   sentence = re.sub(r\"\\'d\", \" would\", sentence)                                  \n #   sentence = re.sub(r\"\\'t\", \" not\", sentence)\n #   sentence = re.sub(r\"won't\", \"will not\", sentence)\n #   sentence = re.sub(r\"can't\", \"can not\", sentence)\n #   return sentence\n#\n#\n#\n#def preprocess(data):             #General Preprocessing\n #   data = contracting(data)      #THIS FUNC DECREASED OUR ACCURACY SO WE COMMENTED IT \n #   data = re.sub(\"[\\-\\\\\\n\\t]\", \" \", data)  #Here we remove all \\n, \\t, - and \\\n #   data = re.sub(\"[^A-Za-z0-9]\", \" \", data)  #Here we remove all the words except alphabets and numbers\n #   data = re.sub('\\s\\s+', ' ', str(data))  #Here we remove all the extra spaces\n #   data = data.lower() #This step is used to convert everything to lower case\n #   return data\n#\n#\n#\n#from nltk.corpus import stopwords\n#import string\n#from nltk.stem.porter import PorterStemmer\n#\n#\n#porter = PorterStemmer()  #USING PORTER ALSO DECREASED OUR ACCURACY\n#\n#\n#def punctuation_remover(sentence: str) -> str:\n #   return sentence.translate(str.maketrans('', '', string.punctuation))\n#\n#   \n#WE USED PUNCTUATION REMOVER TO REMOVE PUNCTUATION BUT WE GOT BETTER RESULTS WITH INCLUDING PUNCTUATIONS.\n#\n#\n#stop_words = stopwords.words('english')\n#def stop_words_remover(k):\n #   k = k.lower()\n #   k = ' '.join([i for i in k.split(' ') if i not in stop_words])\n #   return k\n#\n#\n#    \n#INSTEAD OF USING STOP WORDS REMOVER WE KEPT STOPS WORDS IN TF-IDF THAT WAY WE REMOVED THIS FUNCTION\n#TO USE THE INBUILT FACILITY AVAILABLE WITH TF-IDF\n\n# %% [code]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport re\n\n# %% [code]\ndef tokenizer(text):\n    if text:\n        result = re.findall('[a-z]{2,}', text.lower())\n    else:\n        result = []\n    return result\n\n# %% [code]\n#IN THIS STEP WE DECLARED VARIABLES FOR TFIDF.\ntfidf1 = TfidfVectorizer(max_features = 1000000,ngram_range=(1, 3),tokenizer=tokenizer,stop_words = stopwords.words('english'))\ntfidf2 = TfidfVectorizer(max_features = 1000000,ngram_range=(1, 3),tokenizer=tokenizer,stop_words = stopwords.words('english'))\n#BELOW ARE OUR FAILED TRIALS COMMENTED\n#tfidf3 = CountVectorizer(max_features = 500000,ngram_range=(1, 3),tokenizer=tokenizer,min_df=10)\n#tfidf4 = CountVectorizer(max_features = 500000,ngram_range=(1, 3),tokenizer=tokenizer,min_df=10)\n#tfidf5 = CountVectorizer(max_features = 500000,ngram_range=(1, 3),tokenizer=tokenizer,min_df=10)\n\n# %% [code]\n#WE TRIED APPLYING TF-IDF FOR CATEGORY ALSO, BUT WE GOT BEST RESULT WITH THESE TWO ONLY\ntfidf_description=tfidf1.fit_transform(df_train['item_description'].apply(str))\ntfidf_name = tfidf2.fit_transform(df_train['name'])\n#WE TRIED APPLYING COUNT VECTORIZER FOR NAME BUT TF-IDF WAS BETTER\n\n# %% [code]\n#WHEN COMPARED TO TF-IDF ONEHOT GAVE BETTER ACCURACY TO THE 3 PARTS OF CATEGORY\nfrom sklearn.preprocessing import OneHotEncoder\none_encoder3 = OneHotEncoder(handle_unknown='ignore')\none_encoder4 = OneHotEncoder(handle_unknown='ignore')\none_encoder5 = OneHotEncoder(handle_unknown='ignore')\ntfidf_main = one_encoder3.fit_transform(df_train[['category_main']])\ntfidf_sub1 = one_encoder4.fit_transform(df_train[['category_sub1']])\ntfidf_sub2 = one_encoder5.fit_transform(df_train[['category_sub2']])\n\n# %% [code]\nimport scipy.sparse as spa\n\n# %% [code]\na = spa.hstack((tfidf_description,tfidf_name,tfidf_main,tfidf_sub1,tfidf_sub2))\n\n# %% [code]\none_encoder = OneHotEncoder(handle_unknown='ignore')\none_encoder1 = OneHotEncoder(handle_unknown='ignore')\none_encoder2 = OneHotEncoder(handle_unknown='ignore')\n\n# %% [code]\n#IN THIS STEP WE APPLIED ONEHOT FOR BRAND, SHHIPING, ITEM_CONDITION_ID\nBrand = one_encoder.fit_transform(df_train[['brand_name']])\nShipping = one_encoder1.fit_transform(df_train[['shipping']])\nNewId = one_encoder2.fit_transform(df_train[['item_condition_id']])\n\n# %% [code]\nX = spa.hstack((a,Brand,NewId,Shipping)).tocsr()\n\n# %% [code]\nY = np.log(df_train['price']+1)\n#HERE WE TOOK LOG TO PREVENT FROM GETTING SKEWED RESULTS\n\n# %% [code]\n#THIS IS JUST FOR CALCULATING ACCURACY\n#from sklearn.model_selection import train_test_split\n#X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.05,random_state=0)\n\n# %% [code]\n#BELOW 5 STEPS YOU CAN SEE THE STEP BY STEP DEVELOPEMENT OF OUR HARDWORK ON MODELS\n#WE STARTED WITH LINEAR REGRESSION AND AT LAST WE CONCLUDED WITH THE PAIR OF RIDGE AND LGBM.\n\n# %% [code]\n#from sklearn.model_selection import train_test_split#\n#from sklearn.linear_model import LinearRegression\n#reg = LinearRegression()\n#reg.fit(X,Y)\n\n# %% [code]\nfrom sklearn.linear_model import Ridge\nreg = Ridge(alpha = 1.5)\nreg.fit(X,Y)\n\n# %% [code]\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nmodel1 =  LGBMRegressor(max_depth=10, n_estimators=2000)\nmodel1.fit(X,Y)\n\n# %% [code]\n#import xgboost\n#model2 = xgboost.XGBRegressor(random_state=42)\n#model2.fit(X,Y)\n#xgb DIDNT GAVE GOOD RESULTS\n\n# %% [code]\n#from sklearn.ensemble import RandomForestRegressor\n#regr = RandomForestRegressor( max_features=500000, max_leaf_nodes=64,max_depth=8, random_state=0)\n#regr.fit(X,Y)\n#RANDOM FOREST ALSO DIDNT GIVE BETTER ACCURACY\n\n# %% [code]\n#NOW WE REPEATED SAME PROCESS FOR THE TEST DATA AS WELL\n\n# %% [code]\ndf_test['category_main'], df_test['category_sub1'], df_test['category_sub2'] = zip(*df_test['category_name'].apply(transform_category_name))\n\n# %% [code]\ndf_test['category_main']=df_test['category_main'].fillna(\"No main Categ given\")\ndf_test['category_sub1']=df_test['category_sub1'].fillna(\"No sub1 Categ given\")\ndf_test['category_sub2']=df_test['category_sub2'].fillna(\"No sub2 Categ given\")\ndf_test['brand_name']=df_test['brand_name'].fillna(\"No brand given\")\ndf_test['name'].fillna(df_test['name'].mode()[0],inplace=True)\n\n# %% [code]\ntfidf_t_description = tfidf1.transform(df_test['item_description'].apply(str))\ntfidf_t_name =tfidf2.transform(df_test['name'])\n\n# %% [code]\ntfidf_t_main = one_encoder3.transform(df_test[['category_main']])\ntfidf_t_sub1 = one_encoder4.transform(df_test[['category_sub1']])\ntfidf_t_sub2 = one_encoder5.transform(df_test[['category_sub2']])\n\n# %% [code]\nb = spa.hstack((tfidf_t_description,tfidf_t_name,tfidf_t_main,tfidf_t_sub1,tfidf_t_sub2))\n\n# %% [code]\nTBrand = one_encoder.transform(df_test[['brand_name']])\nTShipping = one_encoder1.transform(df_test[['shipping']])\nTNewId = one_encoder2.transform(df_test[['item_condition_id']])\n\n# %% [code]\nXt = spa.hstack((b,TBrand,TNewId,TShipping)).tocsr()\n\n# %% [code]\npred_t = 0.5*reg.predict(Xt) + 0.5*model1.predict(Xt)\n#\n#\n#BELOW ARE OUR FAILED RATIOS WHICH WE TRIED.\n#pred_t = 0.2*reg.predict(Xt) + 0.8*model1.predict(Xt)\n#pred_t = 0.6*reg.predict(Xt) + 0.4*model1.predict(Xt)\n#\n#\n#NEXT WE TRIED TO COMBINE 4 MODELS BUT IT DIDNT IMPROVE OUR ACCURACY SO WE SETTLED WITH 2 MODELS\n#\n#\n#pred_t = 0.7*reg.predict(Xt) + 0.1*model1.predict(Xt) + 0.1*regr.predict(Xt) + 0.1*model2.predict(Xt)\n#pred_t = 0.8*reg.predict(Xt) + 0.03*model1.predict(Xt) + 0.15*regr.predict(Xt) + 0.02*model2.predict(Xt)\n\n# %% [code]\n#FOLLOWING STEPS ARE JUST TO CREATE A CSV FILE AS OUTPUT IN PROPER FORMAT.\n\n# %% [code]\nfor i,n in enumerate(pred_t):\n  if n<0:\n    pred_t[i]=0\n\n# %% [code]\ndf_test['price']=np.exp(pred_t)-1\n\n# %% [code]\ndf_test[['id','price']].to_csv('final_output_with_combination15.csv', index=False)\n\n# %% [code]\nout=pd.read_csv('./final_output_with_combination15.csv')\nout","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/mercari/train.tsv', sep='\\t')\ndf_test = pd.read_csv('../input/mercari/test.tsv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train shape:',df_train.shape)\nprint('Test shape:',df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_category_name(category_name):\n    try:\n        main, sub1, sub2= category_name.split('/')\n        return main, sub1, sub2\n    except:\n        return np.nan, np.nan, np.nan\n\ndf_train['category_main'], df_train['category_sub1'], df_train['category_sub2'] = zip(*df_train['category_name'].apply(transform_category_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['category_main']=df_train['category_main'].fillna(\"No main Categ given\")\ndf_train['category_sub1']=df_train['category_sub1'].fillna(\"No sub1 Categ given\")\ndf_train['category_sub2']=df_train['category_sub2'].fillna(\"No sub2 Categ given\")\n    df_train['brand_name']=df_train['brand_name'].fillna(\"No brand given\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer(text):\n    if text:\n        result = re.findall('[a-z]{2,}', text.lower())\n    else:\n        result = []\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf1 = TfidfVectorizer(max_features = 1000000,ngram_range=(1, 3),tokenizer=tokenizer)\ntfidf2 = TfidfVectorizer(max_features = 1000000,ngram_range=(1, 3),tokenizer=tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_description=tfidf1.fit_transform(df_train['item_description'].apply(str))\ntfidf_name = tfidf2.fit_transform(df_train['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\none_encoder3 = OneHotEncoder(handle_unknown='ignore')\none_encoder4 = OneHotEncoder(handle_unknown='ignore')\none_encoder5 = OneHotEncoder(handle_unknown='ignore')\ntfidf_main = one_encoder3.fit_transform(df_train[['category_main']])\ntfidf_sub1 = one_encoder4.fit_transform(df_train[['category_sub1']])\ntfidf_sub2 = one_encoder5.fit_transform(df_train[['category_sub2']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.sparse as spa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = spa.hstack((tfidf_description,tfidf_name,tfidf_main,tfidf_sub1,tfidf_sub2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_encoder = OneHotEncoder(handle_unknown='ignore')\none_encoder1 = OneHotEncoder(handle_unknown='ignore')\none_encoder2 = OneHotEncoder(handle_unknown='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Brand = one_encoder.fit_transform(df_train[['brand_name']])\nShipping = one_encoder1.fit_transform(df_train[['shipping']])\nNewId = one_encoder2.fit_transform(df_train[['item_condition_id']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = spa.hstack((a,Brand,NewId,Shipping)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = np.log(df_train['price']+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df_train.corr(), vmin=-1, cmap=\"coolwarm\", annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.05,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nreg = Ridge(alpha = 1.5)\nreg.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nmodel1 =  LGBMRegressor(max_depth=10, n_estimators=2000)\nmodel1.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import xgboost\n#model2 = xgboost.XGBRegressor(random_state=42)\n#model2.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.ensemble import RandomForestRegressor\n#regr = RandomForestRegressor( max_features=500000, max_leaf_nodes=64,max_depth=8, random_state=0)\n#regr.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['category_main'], df_test['category_sub1'], df_test['category_sub2'] = zip(*df_test['category_name'].apply(transform_category_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['category_main']=df_test['category_main'].fillna(\"No main Categ given\")\ndf_test['category_sub1']=df_test['category_sub1'].fillna(\"No sub1 Categ given\")\ndf_test['category_sub2']=df_test['category_sub2'].fillna(\"No sub2 Categ given\")\ndf_test['brand_name']=df_test['brand_name'].fillna(\"No brand given\")\ndf_test['name'].fillna(df_test['name'].mode()[0],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_t_description = tfidf1.transform(df_test['item_description'].apply(str))\ntfidf_t_name =tfidf2.transform(df_test['name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_t_main = one_encoder3.transform(df_test[['category_main']])\ntfidf_t_sub1 = one_encoder4.transform(df_test[['category_sub1']])\ntfidf_t_sub2 = one_encoder5.transform(df_test[['category_sub2']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b = spa.hstack((tfidf_t_description,tfidf_t_name,tfidf_t_main,tfidf_t_sub1,tfidf_t_sub2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TBrand = one_encoder.transform(df_test[['brand_name']])\nTShipping = one_encoder1.transform(df_test[['shipping']])\nTNewId = one_encoder2.transform(df_test[['item_condition_id']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xt = spa.hstack((b,TBrand,TNewId,TShipping)).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pred_t = 0.8*reg.predict(Xt) + 0.03*model1.predict(Xt) + 0.15*regr.predict(Xt) + 0.02*model2.predict(Xt)\npred_t = 0.5*reg.predict(Xt) + 0.5*model1.predict(Xt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,n in enumerate(pred_t):\n  if n<0:\n    pred_t[i]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['price']=np.exp(pred_t)-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[['id','price']].to_csv('fin_output_comb14.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out=pd.read_csv('./fin_output_comb14.csv')\nout","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}